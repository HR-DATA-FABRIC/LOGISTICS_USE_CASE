{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cUExBR03znsF",
        "wb9rTOz_SKhZ",
        "uHIx-nxN0Zif",
        "OgmjHZsvhEKp",
        "3N7A-hlNlB8y",
        "EwQ5W4kpqwYU",
        "xy0i0yTUrJKY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# E-BOOK hands-on materials\n",
        "# Building a Data Driven Infrastructure\n",
        "## How Can Logistics Benefit From Data Science\n"
      ],
      "metadata": {
        "id": "e2KPbQTloCmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Shipment Tracking Use Case Data-set\n",
        "\n",
        "This notebook focusses on **exploratory data analysis (EDA)** in order to prepare a given dataset for the use of **machine learning  (ML) models**.\n",
        "\n",
        "A data scientist is provided with a datawarehouse containing a static dataset of 10999 product shipment tracking observations.\n",
        "<br> This datawarehouse can be downloaded as Train.csv  file from [Kaggle](https://www.kaggle.com/prachi13/customer-analytics).\n",
        "\n",
        "The data scientist’s goal is to put together a custom-made DSI-stack  aimed to develop a ML-based model to forecast  whether ordered products are delivered on time or not."
      ],
      "metadata": {
        "id": "ytIkGgPYoEpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setting up a Colab Jupyter Notebook with scikit-learn\n",
        "--------------------------------------------------------------------------------------------------------------\n",
        "### Shows: “How to Install and import the required data science python packages”\n",
        "--------------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "cUExBR03znsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkc7mL5nNqSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc46f87-5983-4903-9bfc-d7947d0e6bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Installeer de benodigde packages via Jupyter Notebook\n",
        "import sys\n",
        "!{sys.executable} -m pip install numpy pandas matplotlib seaborn scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Data Sciene Toolchain Imports\n",
        "\n",
        "import warnings\n",
        "# supress warnings as output\n",
        "warnings.simplefilter(action='ignore')\n",
        "\n",
        "# Data Science stuff\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "pal = sns.color_palette()\n",
        "\n",
        "## Preprocessing & ML-model Assessment parameters\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "\n",
        "\n",
        "# SKlearn ML-models that can be used to make numerical or binary predictions\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.svm import NuSVC, LinearSVC"
      ],
      "metadata": {
        "id": "vVydCZuQnIH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Getting a basic Understanding of the dataset.\n",
        "----------------------------------------------------------------------------------------------------\n",
        "### Shows: “How to read a warehousing dataset into a DataFrame and  how to explore it visually”\n",
        "------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "wb9rTOz_SKhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('Train.csv')\n",
        "data.sample(10)"
      ],
      "metadata": {
        "id": "SFXAgyGTnYSu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "03c1e51e-225f-45b3-f2a1-bbfcf609231b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-763633583313>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop/Remove ID collumn (1st)\n",
        "data = data.drop('ID', axis=1)"
      ],
      "metadata": {
        "id": "VKEROw53nldt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "n3uWaAFNUkw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "1. Data contains 12 column with 10999 rows\n",
        "2. The data type in each column is appropriate\n",
        "3. No missing values found, let's check to make sure there are no missing values"
      ],
      "metadata": {
        "id": "ivf2nqrDRvky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the meta information as provide with .info()\n",
        "# Determine if there is data missing & Dtype\n",
        "print(\"Missing values:\", data.isna().sum().sum())\n",
        "print(\"Categorical features:\", len(data.select_dtypes('object').columns))\n",
        "print(\"Numerical features:\", len(data.select_dtypes('number').columns))"
      ],
      "metadata": {
        "id": "LD67kt7LnlpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Features"
      ],
      "metadata": {
        "id": "JMaZYw9cTGRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Summary"
      ],
      "metadata": {
        "id": "cHLNlTIrWZzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine the amount of unqiue entries for each feature\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "7NeexNWYTQC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set column to lowercase\n",
        "dats = data.copy()\n",
        "dats.columns=dats.columns.str.lower()\n",
        "\n",
        "# group column names based on type\n",
        "# for numerical data\n",
        "num = ['customer_care_calls', 'customer_rating', 'cost_of_the_product', 'prior_purchases',\n",
        "       'discount_offered', 'weight_in_gms', 'reached.on.time_y.n']\n",
        "data_num = dats[num]\n",
        "\n",
        "# for categorical data\n",
        "cat = ['warehouse_block', 'mode_of_shipment', 'product_importance', 'gender']\n",
        "data_cat = dats[cat]\n",
        "\n",
        "# Generate descriptive statistics on numerical data\n",
        "data_num.describe().applymap('{:,.2f}'.format)"
      ],
      "metadata": {
        "id": "Wz2CaN-STyii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "1. Average *customer_care_calls* 4 times.\n",
        "2. Average *customer_rating* is 3 or medium.\n",
        "3. The *cost_of_the_product* range is 48 - 310 with an average of 210.\n",
        "4. The maximum *prior_purchases* and *discount_offered* are 10 and 65. The average *prior_purchases* are 3 times, while the average *discount_offered* is 13.\n",
        "5. The average *weight_in_gms* is 3634 with a maximum weight of 7846.\n",
        "6. Column *id, customer_care_calls, customer_rating, and prior_purchases*.  seems to be **symmetrically distributed** (the mean and medium values are almost the same).\n",
        "7. Columns *cost_of_the_product, discount_offered, and weight_in_gms* seem to have a **skewed distribution**.\n",
        "8. *reached.on.time_y.n* is boolean/binary columns since the value is 0 or 1, so no need to conclude its simmetricity."
      ],
      "metadata": {
        "id": "WmLSkyI1VC2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation of Numerical Features"
      ],
      "metadata": {
        "id": "7Fng4_Gmo9T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create correlation matrix from numerical features\n",
        "corr_matrix = data[data.select_dtypes('number').columns].corr()\n",
        "\n",
        "# Create mask as to remove the upper half of the corr_matrix\n",
        "mask = np.zeros_like(corr_matrix, dtype=bool)\n",
        "mask[np.triu_indices_from(mask)]= True"
      ],
      "metadata": {
        "id": "hbGgFGdXbgNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the correlation heatmap\n",
        "f, ax = plt.subplots(figsize=(8, 11))\n",
        "\n",
        "heatmap = sns.heatmap(corr_matrix,\n",
        "                      mask = mask,\n",
        "                      square = True,\n",
        "                      linewidths = .5,\n",
        "                      cmap = 'coolwarm',\n",
        "                      cbar_kws = {'shrink': .6,\n",
        "                                'ticks' : [-1, -.5, 0, 0.5, 1]},\n",
        "                      vmin = -1,vmax = 1,\n",
        "                      annot = True,\n",
        "                      annot_kws = {\"size\": 14})\n",
        "\n",
        "#add the column names as labels\n",
        "ax.set_yticklabels(corr_matrix.columns, rotation = 0, fontsize = 10)\n",
        "ax.set_xticklabels(corr_matrix.columns, rotation = 45, fontsize = 8)\n",
        "sns.set_style({'xtick.bottom': True}, {'ytick.left': True})"
      ],
      "metadata": {
        "id": "uDsw52rCdjH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "1. All correlations with the target *reached_on_time* are rather poor\n",
        "2. *discount_offered* has the highest positive correlation: + 0.40\n",
        "3. *cost_of_the_product* has the second highest correlation: + 0.38\n",
        "3. *weight_in_gms* has the highest negative correlation: -0.28"
      ],
      "metadata": {
        "id": "5pn9Yyc9pgs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Features"
      ],
      "metadata": {
        "id": "kV5B0ICapXMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the size of the overall figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Select the categorical features (those of type 'object') from the dataset\n",
        "cat_features = data.select_dtypes('object').columns.values\n",
        "\n",
        "# Loop over each categorical feature\n",
        "for i, cat in enumerate(cat_features):\n",
        "\n",
        "    # Add a subplot for each feature to the overall figure\n",
        "    # The '2, 2' indicates that the subplots will be arranged in a 2x2 grid\n",
        "    # 'i+1' is the index of the current subplot\n",
        "    plt.subplot(2, 2, i+1)\n",
        "\n",
        "    # Create a histogram for the current feature\n",
        "    # 'shrink' reduces the size of the bars by a factor of 0.8 to leave space between them\n",
        "    # 'color' assigns a color to the bars\n",
        "    sns.histplot(data[cat], shrink=0.8, color=pal[i])\n",
        "\n",
        "# Display the figure with all subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tr4qLyzKnl0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Warehouse:** Blocks *A, B, C, D* are equilibrated while block *F* is predominent (1/2 ratio).\n",
        "- **Shipment:** *Flight* and *Road* have similar observations while *Ship* is predominent (1/4 ratio).\n",
        "- **Importance:** There is a majority of *low* and *medium* importances and a minority of *high* importances.\n",
        "- **Genders:** Both classes are balanced."
      ],
      "metadata": {
        "id": "iwU90tQbpxxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical"
      ],
      "metadata": {
        "id": "mkI1tVX9p0UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the size of the figure for the plots\n",
        "plt.figure(figsize=(18, 10))\n",
        "\n",
        "# Get the numeric features from the data, excluding 'Reached.on.Time_Y.N'\n",
        "num_features = data.select_dtypes('number').drop('Reached.on.Time_Y.N', axis=1).columns.values\n",
        "\n",
        "# Iterate over each numeric feature and plot a histogram\n",
        "for i, num in enumerate(num_features):\n",
        "\n",
        "    # Create a subplot for each histogram\n",
        "    plt.subplot(2, 3, i+1)\n",
        "\n",
        "    # Plot a histogram for the numeric feature\n",
        "    # Use the 'bins' argument to specify the number of bins\n",
        "    # This will make the histogram evenly spaced along the x-axis\n",
        "    sns.histplot(data[num], bins=10, color=pal[i])\n",
        "\n",
        "# Adjust the layout to ensure the subplots do not overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qffsO77pbSpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Care Calls:** Sligh positive skewed normal distribution with mode at 4.\n",
        "- **Customer Rating:** Uniform distribution.\n",
        "- **Costs:** 2 picks: smallest around 150, highest around 250.\n",
        "- **Prior Purchases:** Positive skewed normal distribution, mode at 3.\n",
        "- **Discount offered:** Separated into 2 uniform distributions: 0 to 10 is predominent and then small amount from 10 to 65.\n",
        "- **Weight:** 3 zones: high from 1000 to 2000 and from 4000 to 6000. Low from 2000 to 4000."
      ],
      "metadata": {
        "id": "Wryz6BDCp9y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Science Visualization of the whole dataset"
      ],
      "metadata": {
        "id": "bjNx7JR2Nee_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read the dataset\n",
        "dataset = pd.read_csv('Train.csv')\n",
        "\n",
        "# Clean up the column names: remove underscores, capitalize first letter of each word,\n",
        "# and reorder columns to place 'Reached.On.Time Y.N' at the end.\n",
        "new_cols = ['ID'] + [col.replace(\"_\", \" \").title() for col in dataset.columns[1:-1]] + ['Reached.On.Time Y.N']\n",
        "dataset.columns = new_cols\n",
        "\n",
        "# Create a subplot grid for visualization\n",
        "fig, axes = plt.subplots(4, 2, figsize=(12, 14), facecolor='#F2F4F4')\n",
        "\n",
        "# List of column names for countplot\n",
        "columns = [\"Warehouse Block\", \"Mode Of Shipment\", \"Customer Care Calls\", \"Customer Rating\",\n",
        "           \"Prior Purchases\", \"Product Importance\", \"Gender\", \"Reached.On.Time Y.N\"]\n",
        "\n",
        "# List of palette colors for each countplot\n",
        "palettes = ['CMRmap_r', ['#DC143C','#556b2f','#008b8b'], 'cubehelix', \"rocket\",\n",
        "            'viridis', None, ['#800000','#191970'], 'tab20c_r']\n",
        "\n",
        "# List of plot titles for each countplot\n",
        "titles = ['Orders Handled By Each Warehouse Block', 'Number of Orders By Shipment Mode',\n",
        "          'Number of Customer Care Calls Made by Customers', 'Customer Rating Received',\n",
        "          'Number of Prior Purchases Made by Customers', 'Number of Orders Made by Product Importance',\n",
        "          \"Number of Orders Made by Customers' Gender\", 'Number of Orders Based On Arrival Time']\n",
        "\n",
        "# Generate countplots for each column\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < len(columns):\n",
        "        column_counts = dataset[columns[i]].value_counts(ascending=False)\n",
        "        bar_plot = sns.countplot(x=dataset[columns[i]], order=column_counts.index, ax=ax, palette=palettes[i])\n",
        "\n",
        "        # Set plot title and labels\n",
        "        ax.set_title(titles[i], fontsize=12)\n",
        "        column_percentages = column_counts.values * 100 / column_counts.values.sum()\n",
        "        labels = [f\"{count} ({percentage:.2f}%)\" for count, percentage in zip(column_counts, column_percentages)]\n",
        "\n",
        "        # Add labels to each bar manually\n",
        "        for j, p in enumerate(bar_plot.patches):\n",
        "            height = p.get_height()\n",
        "            ax.text(p.get_x()+p.get_width()/2., height + 0.1, labels[j], ha=\"center\", fontsize = 8)\n",
        "\n",
        "# Adjust the layout to ensure the subplots do not overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "twYwSGLuL4Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Analysis"
      ],
      "metadata": {
        "id": "GkizNDxYqDg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio of delayed (1) and not delayed orders (0)\n",
        "data['Reached.on.Time_Y.N'].value_counts() / data['Reached.on.Time_Y.N'].count()\n",
        "data['Reached.on.Time_Y.N'].value_counts()\n",
        "#display(data['Reached.on.Time_Y.N'].sum())"
      ],
      "metadata": {
        "id": "3zyPmOHXp4-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()\n",
        "display(df)"
      ],
      "metadata": {
        "id": "K25EOT7XIFDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target classes are slightly unbalanced."
      ],
      "metadata": {
        "id": "D_quuMdkqRL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Preprocessing, feature analysis and  ML-model performance assessment\n",
        "--------------------------------------------------------------------------------------------------------------------------------\n",
        "### Shows: “How to perform data preprocessing to enable ML-learning & Feature analysis”\n",
        "--------------------------------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "uHIx-nxN0Zif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: One-HOT encoding + Scaling"
      ],
      "metadata": {
        "id": "K342R48PqVgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def preprocess_inputs(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Drop/Remove ID collumn (1st)\n",
        "    # df = df.drop('ID', axis=1)\n",
        "\n",
        "    # exluding categorical data\n",
        "    #df= df.select_dtypes(exclude=['object'])\n",
        "\n",
        "    # One-HOT encoding of the categorical data\n",
        "    df = pd.concat([df, pd.get_dummies(df['Warehouse_block'])], axis=1).drop('Warehouse_block', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Mode_of_Shipment'])], axis=1).drop('Mode_of_Shipment', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Product_importance'])], axis=1).drop('Product_importance', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Gender'], prefix='Sex')], axis=1).drop('Gender', axis=1)\n",
        "\n",
        "    # Split dataset into INPUT: X  /  OUTPUT: target\n",
        "    X = df.drop('Reached.on.Time_Y.N', axis=1)\n",
        "    target = df['Reached.on.Time_Y.N']\n",
        "\n",
        "    # Standardizing the features\n",
        "    scaler = StandardScaler()\n",
        "    scaled_features = pd.DataFrame(scaler.fit_transform(X))\n",
        "\n",
        "    return scaled_features, target, X"
      ],
      "metadata": {
        "id": "occUJJARope9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 10 samples of the One-HOT encoded dataset\n",
        "# X is One-HOT encoded data\n",
        "pdata, target, hot = preprocess_inputs(data)\n",
        "display(hot.sample(10))"
      ],
      "metadata": {
        "id": "AtESHMSd_yoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show 1- samples of the scaled, One-HOT encoded dataset\n",
        "# pdata is One-HOT encoded + scaled data\n",
        "pdata, target, hot = preprocess_inputs(data)\n",
        "display(pdata.sample(10))"
      ],
      "metadata": {
        "id": "2u5qw6BszYWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature analysis: Logistic Regression fitting"
      ],
      "metadata": {
        "id": "3n40rUjHPffR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pdata is One-HOT encoded + scaled data\n",
        "pdata, target, hot = preprocess_inputs(data)\n",
        "\n",
        "# Split the train data into train and test\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "train_test_split(pdata, target, shuffle=True, train_size=0.8, random_state=0)"
      ],
      "metadata": {
        "id": "i9ALDzVR64P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model specific Imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "logreg_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = logreg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature coefficients\n",
        "feature_importance = logreg_model.coef_[0]\n",
        "#print(\"Feature Coefficients:\", feature_importance)\n",
        "\n",
        "labels = hot.columns.tolist()\n",
        "\n",
        "# Assuming 'logreg_model' is your trained model\n",
        "coefficients = logreg_model.coef_[0]\n",
        "\n",
        "# Use 'labels' variable for features\n",
        "feature_importance = pd.DataFrame({'Feature': labels, 'Importance': np.abs(coefficients)})\n",
        "feature_importance = feature_importance.sort_values('Importance', ascending=True)\n",
        "\n",
        "ax = feature_importance.plot(x='Feature', y='Importance', kind='barh', figsize=(10, 6))\n",
        "ax.set_title('Logistic Regression Feature Importance Analysis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Of2cowFsDh3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature analysis: Decision Tree fitting"
      ],
      "metadata": {
        "id": "TvMHVPuOk7LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a decision tree model\n",
        "tree_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "#print(f\"Decision Tree Accuracy: {accuracy_tree:.2f}\")\n",
        "\n",
        "# Plot feature importances\n",
        "feature_importance_tree = tree_model.feature_importances_\n",
        "\n",
        "# Assuming 'labels' variable for features\n",
        "feature_importance_tree_df = \\\n",
        "pd.DataFrame({'Feature': labels, 'Importance': np.abs(feature_importance_tree)})\n",
        "\n",
        "feature_importance_tree_df = feature_importance_tree_df.sort_values('Importance', ascending=True)\n",
        "\n",
        "ax_tree = feature_importance_tree_df.plot(x='Feature', y='Importance', kind='barh', figsize=(8, 4))\n",
        "ax_tree.set_title('Decision Tree Feature Importance Analysis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mey9wkSK_XM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Selecting the best available supervised-learning ML-model\n",
        "----------------------------------------------------------------------------------------------------\n",
        "### Shows: “How to import, train, evaluate & select  interpretable  ML-models using Sklearn”\n",
        "------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "OgmjHZsvhEKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "3N7A-hlNlB8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # One-hot encoding of the categorical data\n",
        "    df = pd.concat([df, pd.get_dummies(df['Warehouse_block'])], axis=1).drop('Warehouse_block', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Mode_of_Shipment'])], axis=1).drop('Mode_of_Shipment', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Product_importance'])], axis=1).drop('Product_importance', axis=1)\n",
        "    df = pd.concat([df, pd.get_dummies(df['Gender'], prefix='Sex')], axis=1).drop('Gender', axis=1)\n",
        "\n",
        "    # Split X and y\n",
        "    X = df.drop('Reached.on.Time_Y.N', axis=1)\n",
        "    y = df['Reached.on.Time_Y.N']\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, train_size=0.8, random_state=0)\n",
        "\n",
        "    # Scale X\n",
        "    scaler = StandardScaler()\n",
        "    X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)\n",
        "    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
        "\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "anePAmYcqImE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing & Training Multiple models"
      ],
      "metadata": {
        "id": "EwQ5W4kpqwYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Available predicting ML-models from Sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.svm import NuSVC, LinearSVC"
      ],
      "metadata": {
        "id": "hIbPRaVJCxI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SKlearn ML-models\n",
        "models = {\n",
        "    \"Logistic\": LogisticRegression(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"RF\": RandomForestClassifier(),\n",
        "    \"SVC\": SVC(probability=True),\n",
        "    \"GNB\": GaussianNB(),\n",
        "    \"CART\": DecisionTreeClassifier(),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "    \"HGB\": HistGradientBoostingClassifier(),\n",
        "    \"Ada\": AdaBoostClassifier(n_estimators=100, random_state=0),\n",
        "    \"XGB\": XGBClassifier(),\n",
        "    ##\"SGD\": SGDClassifier(loss='squared_hinge'),\n",
        "    \"GBC\": GradientBoostingClassifier(),\n",
        "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
        "    ##\"RC\": RidgeClassifier(),\n",
        "    ##\"PAC\": PassiveAggressiveClassifier(),\n",
        "    ##\"P\": Perceptron(),\n",
        "    \"ETC\": ExtraTreesClassifier(),\n",
        "    \"BC\": BaggingClassifier(),\n",
        "    \"LS\": LabelSpreading(),\n",
        "    \"LP\": LabelPropagation(),\n",
        "    ##\"GPC\": GaussianProcessClassifier(), # takes a long time to train\n",
        "    ##\"RCCV\": RidgeClassifierCV(),\n",
        "    \"NuSVC\": NuSVC(probability=True),\n",
        "    ##\"LSVC\": LinearSVC()\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize progress bar\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "pbar = tqdm(total=len(models))\n",
        "\n",
        "# Train each model and update progress bar\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()  # Start time\n",
        "    print(f\"Training model: {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()  # End time\n",
        "    duration = end_time - start_time  # Calculate duration\n",
        "    # Print duration as a whole number\n",
        "    print(f\"Finished training model: {name} in {int(duration)} seconds\")\n",
        "    pbar.update(1)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()"
      ],
      "metadata": {
        "id": "Q2ySY7bCC-JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming trained_ensemble_models and X_test, y_test are defined\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"-----------------------------\")\n",
        "    print(f\"Classification report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-----------------------------\")\n",
        "    print(\"-----------------------------\")\n"
      ],
      "metadata": {
        "id": "TlVuhBdJb3hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Compute AUC for each model and store in a dictionary\n",
        "auc_dict = {}\n",
        "for name, model in models.items():\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    auc_dict[name] = (roc_auc, fpr, tpr)\n",
        "\n",
        "# Sort the models by AUC from high to low\n",
        "sorted_models = sorted(auc_dict.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "# Plot size\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Plot ROC curve for each model, in sorted order\n",
        "for name, (roc_auc, fpr, tpr) in sorted_models:\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Diagonal line\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "\n",
        "# Set plot labels and legend\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wu76aYPPhHOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_list = list(models.values())\n",
        "print(model_list[1])"
      ],
      "metadata": {
        "id": "m6wspiAeCuiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = models['RF'].predict(X_test)\n",
        "y_pred-y_test"
      ],
      "metadata": {
        "id": "r6QCQ1d3GRn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Results"
      ],
      "metadata": {
        "id": "xy0i0yTUrJKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "    f1 = f1_score(y_test, y_pred) * 100\n",
        "    recall = recall_score(y_test, y_pred) * 100\n",
        "    precision = precision_score(y_test, y_pred) * 100\n",
        "\n",
        "    print(name + \"    Accuracy: {:.2f} %\".format(accuracy))\n",
        "    print(\"            F1 Score: {:.2f} %\".format(f1))\n",
        "    print(\"              Recall: {:.2f} %\".format(recall))\n",
        "    print(\"           Precision: {:.2f} %\".format(precision))\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    results.append(confusion_matrix(y_test, y_pred))\n",
        "    # Calculate the score for this model\n",
        "    scores[name] = accuracy + f1 + recall + precision\n",
        "\n",
        "# Find the model with the highest score\n",
        "best_model = max(scores, key=scores.get)\n",
        "print(f\"The best model is {best_model} with a score of {scores[best_model]}\")"
      ],
      "metadata": {
        "id": "suUdkICiJ7G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axs = plt.subplots(6, 3, figsize=(10, 16))\n",
        "\n",
        "for i, name in enumerate(list(models.keys())):\n",
        "    ax = plt.subplot(5, 4, i + 1)\n",
        "    sns.heatmap(results[i], annot=True, square=True, cbar=False,\n",
        "                xticklabels=['No delay', 'Delay'], yticklabels=['No delay', 'Delay'], cmap='Reds', fmt='10.0f', ax=ax)\n",
        "    plt.title(name)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "\n",
        "# Adjust the spacing between the subplots\n",
        "plt.subplots_adjust(wspace=0.4, hspace=0.0)\n",
        "\n",
        "# Adjust the layout to en\n",
        "# plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OiIDXVPgHeul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scores are low, it is not always possible to get high scores when features are not well correlated, and this is what we saw in the heatmap.\n",
        "\n",
        "This dataset also seems to be fictive since some classes are perfectly balanced. In any case, this was a good practice."
      ],
      "metadata": {
        "id": "Yd1CdG07rdZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base models\n",
        "base_models = [\n",
        "    ('logistic_regression', LogisticRegression()),\n",
        "    ('knn', KNeighborsClassifier()),\n",
        "    ('random_forest', RandomForestClassifier()),\n",
        "    ('svc', SVC(probability=True)),\n",
        "    ('gnb', GaussianNB()),\n",
        "    ('cart', DecisionTreeClassifier()),\n",
        "    ('hgb', HistGradientBoostingClassifier()),\n",
        "    ('ada', AdaBoostClassifier(n_estimators=100, random_state=0)),\n",
        "    ('mlpc', MLPClassifier(activation='tanh', solver='lbfgs', alpha=0.001, hidden_layer_sizes=(8, 2), random_state=1,max_iter=20000, early_stopping=True)),\n",
        "]\n",
        "\n",
        "# Define ensemble models\n",
        "models = {\n",
        "    \"VC\": VotingClassifier(estimators=base_models, voting='soft'),  # 'soft' voting returns the class with the highest sum of predicted probabilities\n",
        "    \"SC\": StackingClassifier(estimators=base_models, final_estimator=LogisticRegression()),  # final_estimator is used to combine the base models\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize progress bar\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "pbar = tqdm(total=len(models))\n",
        "\n",
        "# Train each model and update progress bar\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()  # Start time\n",
        "    print(f\"Training model: {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()  # End time\n",
        "    duration = end_time - start_time  # Calculate duration\n",
        "    print(f\"Finished training model: {name} in {int(duration)} seconds\")  # Print duration as a whole number\n",
        "    pbar.update(1)\n",
        "\n",
        "# Close progress bar\n",
        "pbar.close()"
      ],
      "metadata": {
        "id": "HOHM87R0VN9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "scores = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
        "    f1 = f1_score(y_test, y_pred) * 100\n",
        "    recall = recall_score(y_test, y_pred) * 100\n",
        "    precision = precision_score(y_test, y_pred) * 100\n",
        "    print(name + \"    Accuracy: {:.2f} %\".format(accuracy))\n",
        "    print(\"            F1 Score: {:.2f} %\".format(f1))\n",
        "    print(\"              Recall: {:.2f} %\".format(recall))\n",
        "    print(\"           Precision: {:.2f} %\".format(precision))\n",
        "    print(\"-----------------------------\")\n",
        "    results.append(confusion_matrix(y_test, y_pred))\n",
        "    # Calculate the score for this model\n",
        "    scores[name] = accuracy + f1 + recall + precision\n",
        "\n",
        "# Find the model with the highest score\n",
        "best_model = max(scores, key=scores.get)\n",
        "\n",
        "print(f\"The best model is {best_model} with a score of {scores[best_model]}\")"
      ],
      "metadata": {
        "id": "jQU9roMkKJUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Assuming X_test and y_test are your testing data and labels\n",
        "\n",
        "# Plot size\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Predict probabilities for the positive class\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Compute ROC curve and ROC area\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot the ROC curve\n",
        "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Diagonal line\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "\n",
        "# Set plot labels and legend\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v7S8Ab-GfDhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}